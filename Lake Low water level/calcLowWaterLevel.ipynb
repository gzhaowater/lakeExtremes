{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd32b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216.010752\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "########## Check memory##########\n",
    "import os\n",
    "import psutil\n",
    "process = psutil.Process(os.getpid())\n",
    "print(process.memory_info().rss/1e6)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d84995",
   "metadata": {},
   "source": [
    "# 1. calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f42ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## area data is monthly lake area from the GLEV dataset (https://doi.org/10.1038/s41467-022-31125-6) \n",
    "## with updated area extraction algorithm from Zhao et al. (2020) (https://doi.org/10.1016/j.rse.2020.112104)\n",
    "## ice data is monthly ice cover fraction from the GLEV dataset (https://doi.org/10.1038/s41467-022-31125-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c94372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_data = np.load('out_0_area.npy', mmap_mode='r')\n",
    "ice_data = np.load('out_0_ice.npy', mmap_mode='r')\n",
    "\n",
    "lake_infos_path = 'HydroLakes/Hylak_info.npy'\n",
    "lake_infos = np.load(lake_infos_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc1b5861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75, 105, 136, 166, 197, 228, 258, 289, 319, 350, 381, 410, 440, 470, 501, 531, 562, 593, 623, 654]\n",
      "442\n",
      "[60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]\n",
      "13455\n"
     ]
    }
   ],
   "source": [
    "months1 = pd.date_range('1984-03-01','2020-12-31',freq='MS')\n",
    "months2 = pd.date_range('1984-03-01','2020-12-31',freq='M')\n",
    "months = [c1+(c2-c1)/2 for c1,c2 in zip(months1, months2)]\n",
    "months_ref = [(c-np.datetime64('1984-01-01')).days for c in months]\n",
    "\n",
    "print(months_ref[0:20])\n",
    "print(len(months_ref))\n",
    "\n",
    "days = pd.date_range('1984-03-01','2020-12-31',freq='D')\n",
    "days_ref = [(c-np.datetime64('1984-01-01')).days for c in days]\n",
    "print(days_ref[0:60])\n",
    "print(len(days_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "274e02aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_df = pd.DataFrame([])\n",
    "day_df['day'] = pd.date_range('1984-01-01','2020-12-31',freq='D')\n",
    "day_df['month'] = day_df['day'].dt.strftime('%Y-%m-01')\n",
    "day_df['mth'] = day_df['day'].dt.month\n",
    "day_df['one'] = 1\n",
    "mdays = day_df.groupby('month')['one'].apply(list).values\n",
    "mdays = [np.array(c) for c in mdays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d4500bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: array([  306,   307,   308, ..., 13117, 13118, 13119]), 2: array([  337,   338,   339, ..., 13146, 13147, 13148]), 3: array([    0,     1,     2, ..., 13177, 13178, 13179]), 4: array([   31,    32,    33, ..., 13207, 13208, 13209]), 5: array([   61,    62,    63, ..., 13238, 13239, 13240]), 6: array([   92,    93,    94, ..., 13268, 13269, 13270]), 7: array([  122,   123,   124, ..., 13299, 13300, 13301]), 8: array([  153,   154,   155, ..., 13330, 13331, 13332]), 9: array([  184,   185,   186, ..., 13360, 13361, 13362]), 10: array([  214,   215,   216, ..., 13391, 13392, 13393]), 11: array([  245,   246,   247, ..., 13421, 13422, 13423]), 12: array([  275,   276,   277, ..., 13452, 13453, 13454])}\n"
     ]
    }
   ],
   "source": [
    "day_df1 = day_df[day_df['day'] >= '1984-03-01'].copy().reset_index(drop=True)\n",
    "\n",
    "idx = {}\n",
    "for mth in np.arange(1,13,1):\n",
    "    idx[mth] = day_df1[day_df1['mth'] == mth].index.values\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03367a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for k in range(1, len(area_data)):\n",
    "    \n",
    "    hylak_id = k + 1\n",
    "        \n",
    "    dates = days\n",
    "    years = [c.year for c in dates]\n",
    "    areas = area_data[k]\n",
    "    \n",
    "    ######### Daily ice cover ##########\n",
    "    ice = ice_data[hylak_id-2]\n",
    "    tmp = copy.deepcopy(mdays)\n",
    "    for j in range(444):\n",
    "        length = len(tmp[j])\n",
    "        if ice[j] == 0:\n",
    "            tmp[j] = np.zeros(length)\n",
    "        elif ice[j] < 1 and j>=1 and j<443:\n",
    "            if ice[j-1] == 1:\n",
    "                tmp[j][int(length-(1-ice[j])*length):length] = np.zeros(length-int(length-(1-ice[j])*length))\n",
    "            if ice[j+1] == 1:\n",
    "                tmp[j][0:int((1-ice[j])*length)] = np.zeros(int((1-ice[j])*length))\n",
    "        elif ice[j] < 1 and j==0:\n",
    "            if ice[j+1] == 0:\n",
    "                tmp[j][int(length-(1-ice[j])*length):length] = np.zeros(length-int(length-(1-ice[j])*length))\n",
    "            if ice[j+1] == 1:\n",
    "                tmp[j][0:int((1-ice[j])*length)] = np.zeros(int((1-ice[j])*length))\n",
    "        elif ice[j] < 1 and j==443:\n",
    "            if ice[j-1] == 0:\n",
    "                tmp[j][0:int((1-ice[j])*length)] = np.zeros(int((1-ice[j])*length))\n",
    "            if ice[j-1] == 1:\n",
    "                tmp[j][int(length-(1-ice[j])*length):length] = np.zeros(length-int(length-(1-ice[j])*length))\n",
    "\n",
    "    icefree = [1-num for sublist in tmp[2:] for num in sublist]\n",
    "    \n",
    "    ######### percentiles 10, 90 #########\n",
    "    area_daily = np.interp(days_ref, months_ref, areas)\n",
    "\n",
    "    p10 = {}\n",
    "    p90 = {}\n",
    "    for mth in np.arange(1,13,1):\n",
    "        p10[mth], p90[mth] = np.quantile(area_daily[idx[mth]], [0.1,0.9]) \n",
    "        \n",
    "    p10 = np.concatenate([mdays[i]*p10[i%12+1] for i in range(444)[2:]])\n",
    "    p90 = np.concatenate([mdays[i]*p90[i%12+1] for i in range(444)[2:]])\n",
    "    \n",
    "    ######### area data extremes ##########\n",
    "    lows = (area_daily <= p10).astype(int)\n",
    "    highs = (area_daily >= p90).astype(int)\n",
    "    \n",
    "    lows = [lows[i]*icefree[i] for i in range(len(lows))]\n",
    "    highs = [highs[i]*icefree[i] for i in range(len(highs))]\n",
    "    \n",
    "    low_intensity = [(p10[i]-area_daily[i])*lows[i] for i in range(len(area_daily))]\n",
    "    high_intensity = [(area_daily[i]-p90[i])*highs[i] for i in range(len(area_daily))]\n",
    "    \n",
    "    year_idx = np.unique(years, return_index=True)[1][1:]\n",
    "    year_yr = [np.mean(c) for c in np.split(years, year_idx)]\n",
    "    icefree_yr = [np.sum(c) for c in np.split(icefree, year_idx)]\n",
    "    low_yr = [np.sum(c) for c in np.split(lows, year_idx)]\n",
    "    high_yr = [np.sum(c) for c in np.split(highs, year_idx)]\n",
    "    \n",
    "    low_inty_yr = [np.mean(c) for c in np.split(low_intensity, year_idx)]\n",
    "    high_inty_yr = [np.mean(c) for c in np.split(high_intensity, year_idx)]\n",
    "    hylak_id_yr = np.ones(len(year_yr))*hylak_id\n",
    "    \n",
    "    output.append([hylak_id_yr, year_yr, icefree_yr, low_yr, high_yr, low_inty_yr, high_inty_yr])\n",
    "    \n",
    "    if k%100 == 0:\n",
    "        print(k, datetime.now(), process.memory_info().rss/1e6)\n",
    "    \n",
    "np.save('out_1_lake_area_extremes_ss.npy', np.float32(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eeb91f",
   "metadata": {},
   "source": [
    "# 2. summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3d95ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.load('out_1_lake_area_extremes_ss.npy', mmap_mode='r')\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "364abc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(output.transpose(0,2,1).reshape(-1,7), \n",
    "                   columns=['hylak_id','year','icefree','lows','highs','low_inty','high_inty'])\n",
    "\n",
    "def period(year):\n",
    "    if year>=1985 and year<=1999:\n",
    "        prd = 'X1980_90s'\n",
    "    elif year>=2000 and year<=2009:\n",
    "        prd = 'X2000s'\n",
    "    elif year>=2010 and year<=2019:\n",
    "        prd = 'X2010s'\n",
    "    else:\n",
    "        prd = 'Ot'\n",
    "    return prd\n",
    "df['period'] = df['year'].apply(period)\n",
    "\n",
    "df_avg = df[(df['period'].isin(['X1980_90s','X2000s','X2010s']))] \\\n",
    "            .groupby(['hylak_id','period']).mean().drop(columns=['year']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cef8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_low_freq = df_avg.pivot(index='hylak_id',columns='period',values='lows')\n",
    "df_low_freq.columns = [c+'_LowEx_Freq' for c in df_low_freq.columns]\n",
    "\n",
    "df_high_freq = df_avg.pivot(index='hylak_id',columns='period',values='highs')\n",
    "df_high_freq.columns = [c+'_HighEx_Freq' for c in df_high_freq.columns]\n",
    "\n",
    "df_low_int = df_avg.pivot(index='hylak_id',columns='period',values='low_inty')\n",
    "df_low_int.columns = [c+'_LowEx_Int' for c in df_low_int.columns]\n",
    "\n",
    "df_high_int = df_avg.pivot(index='hylak_id',columns='period',values='high_inty')\n",
    "df_high_int.columns = [c+'_HighEx_Int' for c in df_high_int.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8067bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([df_low_freq, df_high_freq, df_low_int, df_high_int], axis=1).reset_index()\n",
    "all_data.to_csv('out_1_lakeArea_extremes_ss.csv.zip', index=False, \n",
    "                compression=dict(method='zip', archive_name='out_1_lakeArea_extremes_ss.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
